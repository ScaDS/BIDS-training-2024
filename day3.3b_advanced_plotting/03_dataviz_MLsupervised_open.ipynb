{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) ## suppress annoying deprecation warnings\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn.objects as so\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Machine Learning insights by Data Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns for better axis labels in plots\n",
    "\n",
    "col_rename = {\n",
    "\t'tavg': 'Temp_Avg_°C',\n",
    "\t'tmax': 'Temp_Max_°C',\n",
    "\t'tmin': 'Temp_Min_°C',\n",
    "\t'rhum': 'Rel_Humidity_%',\n",
    "\t'coco': 'Condition',\n",
    "\t'wspd': 'Wind_Speed_kmh',\n",
    "\t'prcp': 'Precipation_mm',\n",
    "\t'wdir': 'Wind_Direction_°',\n",
    "\t'pres': 'Air_pressure_hPa',\n",
    "\t'dwpt': 'Dew_point_°C'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reload data\n",
    "weather_df = pd.read_csv('global_weather.csv', parse_dates=['time'], dtype={'wmo':str, 'station':str}) \n",
    "weather_df = weather_df.dropna()\n",
    "\n",
    "weather_df.rename(columns=col_rename, inplace=True)\n",
    "weather_df = weather_df.assign(Continent = weather_df[\"timezone\"].str.split('/').str[0])  ## Get continent from timezone column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting feature correlation: first try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_corr = weather_df.select_dtypes(include='number').corr() ## Calculate (Pearson) correlation for all numerical features in dataframe\n",
    "\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "#!# sns.??(weather_corr, ## seaborn.object no function for heatmaps yet\n",
    "\t\t\tcbar_kws={\"label\": \"Pearson correlation coefficient\", \"shrink\": 0.6} ## Label adjustments\n",
    "\t\t\t) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with standard correlation heatmap: colormap not suitable and features not ordered by similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(weather_corr,                                            ## Clustermap will cluster the features by similarity\n",
    "\t\t\t    cmap='vlag',center=0, vmin=-1, vmax=1,                  ## Colormap: correlations range from -1 to +1 and have fixed midpoint at 0                                  \n",
    "\t\t\t\tcbar_kws={\"label\": \"Pearson correlation coefficient\"},  ## Color legend description\n",
    "                cbar_pos = (0.05, 0.45, 0.03, 0.2)\t\t\t\t\t\t## Change the weird default position of legend\n",
    "\t\t\t\t).ax_row_dendrogram.set_visible(False)\t\t\t\t\t## Dendrogram of feature similarity is identical (symmetric matrix); we can omit it on one side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can be seen in the correlation heatmap?\n",
    "(1) features with (almost) identical information (redundancy) <br>\n",
    "(2) association of learned embeddings (t-SNE, PCA) with features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML show case: predict the manuel text annotation of the weather by the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df['Condition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For simplicity reducing to Top categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df_red = weather_df[\n",
    "\tweather_df['Condition'].isin( \n",
    "\t\tweather_df['Condition'].value_counts()[0:6].index  ## Top categories\n",
    "\t\t) \n",
    "\t]\n",
    "\n",
    "weather_df_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = weather_df_red.select_dtypes(include='number') \t## Define features\n",
    "#!# y = weather_df_red[??]  \t\t\t\t## Define target variable\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=5,class_weight=\"balanced\", max_leaf_nodes = 6) ## Define a simple decision tree\n",
    "\n",
    "clf = clf.fit(X, y)\t\t## training\n",
    "\n",
    "plt.figure(figsize=(30,12))  ## Plot the full decision tree\n",
    "anno = tree.plot_tree(\tclf, \t\t\t\t\t\t\t\t\t## Decision tree\n",
    "\t\t\t   \tfeature_names=clf.feature_names_in_.tolist(),\t## Features names\n",
    "\t\t\t   \tclass_names = clf.classes_.tolist(),\t\t\t## Weather condition text\n",
    "\t\t\t\tfilled = True,\t\t\t\t\t\t\t\t\t## Colored by class decision\n",
    "\t\t\t\timpurity =True,\t\t\t\t\t\t\t\t## For simiplicity exclude impurity values at splits\n",
    "\t\t\t\tprecision=1,\t\t\t\t\t\t\t\t\t## decimal precision\n",
    "\t\t\t\tfontsize=12)\t\t\t\t\t\t\t\t\t## Fontsize for readibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights into more complex ML models: Random Forest\n",
    "### Plotting results: confusion matrix\n",
    "Important for Confusion Matrix visualization is color + number per entry for pattern exploration and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=100,  class_weight=\"balanced\", max_leaf_nodes=20) ## What happens if we negelect class imbalance?\n",
    "#!# clf_rf = RandomForestClassifier(n_estimators=100,  class_weight=None, max_leaf_nodes=20) ## What happens if we negelect class imbalance?\n",
    "\n",
    "\n",
    "output = cross_validate(clf_rf, X, y, cv=5, scoring = 'accuracy', return_estimator =True) ## RandomForest is non-deterministic ML -> Cross-validation for more robust results\n",
    "\n",
    "clf_rf_1 = output['estimator'][0] ## Select a learned RF from the cross-validation\n",
    "\n",
    "y_pred = clf_rf_1.predict(X)\n",
    "labels = clf_rf_1.classes_.tolist()\n",
    "cm = confusion_matrix(y, y_pred, labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(ax=ax) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting explainability from feature importance\n",
    "### Extracting importance from learned models across all cross validations for robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(X.columns.to_list(), index = X.columns.to_list(), columns=['feature']) ## Empty Data frame definition\n",
    "\n",
    "for idx,estimator in enumerate(output['estimator']):  ## For each tree in RF\n",
    "    feature_importances = feature_importances.join(         \n",
    "            pd.DataFrame(estimator.feature_importances_ ,             ## Calculated feature importance\n",
    "                        index = estimator.feature_names_in_.tolist(), ## Feature Names\n",
    "                        columns=['importance_cv'+str(idx+1)])         ## Save from which CV split\n",
    "            )\n",
    "\n",
    "# feature_importances\n",
    "#!# feature_importances_long = pd.??( feature_importances, stubnames=\"importance_cv\",i=\"feature\",j=\"cv\") \n",
    "\n",
    "feature_importances_long.head() ## We need typically long format for plotting in grammar of graphics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show importance as bar chart including errorbars\n",
    "### Trick: flip x and y axis for readibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "#!#    so.Plot(feature_importances_long,x=??,y=??)\n",
    "    .add(so.Bar(), so.Agg())\t\t\t\t## Bar plot showing the average\n",
    "\t.add(so.Range(), so.Est(errorbar=\"sd\"))\t## Whiskers showing the standard deviation\n",
    "\t.layout(size=(8, 6))\t\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are problems with bar charts and errorbars: https://doi.org/10.1371/journal.pbio.1002128\n",
    "### Better show points/dots for every trained model (sample) and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    so.Plot(feature_importances_long,x='importance_cv',y='feature')\n",
    "\t.add(so.Dot(pointsize=5), so.Shift(y=.0), so.Jitter(.5)) ## Jitter and Shift avoid overplotting\n",
    "#!# .add(so.??(color=\"red\"), so.Agg())\t\t\t\t\t## Show a dash with the average\n",
    "#!# .add(so.??(color=\"red\"), so.Est(errorbar=\"sd\"))\t\t## Show the range of the standard deviation \n",
    "\t.layout(size=(8, 6))\t\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datavizenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
